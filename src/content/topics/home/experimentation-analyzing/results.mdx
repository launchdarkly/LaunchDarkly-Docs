---
path: /home/analyzing-experiments/results
title: Reading experiment results
description: This topic explains how to read and understand an experiment's results tab.
published: true
tags: ['analyze', 'result', 'experiment', 'experimentation', 'enterprise']
---

## Overview

This topic explains how to read and understand an experiment's results tab.

An experiment's **Results** tab provides information about each variation's performance in the experiment, how it compares to the other variations, and which variations are likely to be best out of all the tested options. Understanding how to read this tab can help you make informed decisions about when to edit, stop, or choose a winning variation for your experiments.

An experiment's **Results** tab includes the following sections:

- [Attribute filters](#attribute-filters)
- [Traffic count](#traffic-count)
- [Sample ratio mismatch](#sample-ratio-mismatch)
- [Probability chart](#probability-chart)
- [Results table](#results-table)

## Attribute filters

Attribute filters allow you to slice your experiment results by certain cross-sections of your experiment audience. For example, if you're interested in how variations perform for end users in Canada as compared to end users in Mexico, you could slice your data by the "country" context attribute. This would allow you to compare performance of variations in each individual country, in addition to seeing the variation that performs best overall.

To be able to slice results by attribute, you must add attributes in the "Select metrics" step when you create an experiment. To learn how, read [Building experiments](/home/creating-experiments#building-experiments).

You can then slice your results by the different values of one of those attributes. 

To slice experiment results by attribute:

1. Navigate to your experiment's **Results** tab.
2. Select one or more attribute values in the "Attribute filters" section. The "Experiment traffic from:" section updates to display results from all attributes, and from the attribute value you selected:

![Experiment results sliced by attribute.](experiment-results-by-attribute.png)

3. Scroll down to the "Probability report" section to view the probability to be best for all attributes, and for the attribute you selected.

<Callout intent="info">
<CalloutTitle>You cannot slice results for anonymous contexts or private attributes</CalloutTitle>
<CalloutDescription>

Private attributes and contexts with the <code>anonymous</code> property set to <code>true</code> are not available for data slicing. Private attributes and anonymous context attributes do not appear in the "Attribute filters" menus.

</CalloutDescription>
</Callout>

## Traffic count

The traffic count table displays how many unique contexts have encountered each variation within the experiment:

![An experiment's traffic count table showing each variation's total number of contexts.](experiment-traffic-count.png)

Here's how the table counts unique contexts:

- The table only counts contexts with the same context kind as the experiment's randomization unit. For example, if your experiment's randomization unit is `user`, then the table counts only `user` contexts. The table won't include any `device` or `organization` contexts that are in the experiment. To learn more, read [Randomization units](/home/creating-experiments/allocation#randomization-units).
- If the same context is in the experiment multiple times, the table counts the context only once.

## Sample ratio mismatch

If your experiment has a sample ratio mismatch, it means there may be an issue with your JavaScript-based SDKs. To learn more, read [Understanding sample ratios](/home/analyzing-experiments/sample-ratios).

## Probability chart

An experiment's probability chart provides a visual representation of the performance of each variation tested in an experiment:

![An experiment's probability chart.](experiment-probability-chart.png)

To hide any of the variations from the probability chart, uncheck the box next to the variation's name:

![An experiment's probability chart with a variation unchecked.](experiment-probability-chart-unchecked-callout.png)

To hide the chart completely, click **Hide probability chart**.

Each Experimentation probability chart is unique, and how you interpret the results depends on what metric you're measuring and the hypothesis of your experiment. However, we've provided some general guidelines for interpreting experiment results.

<LearnMore>

You can also use the REST API: [Get experiment results](https://apidocs.launchdarkly.com/tag/Experiments-(beta)#operation/getExperimentResults)

</LearnMore>

### The x-axis

The horizontal x-axis displays the unit of the primary metric included in the experiment. For example, if the metric is measuring revenue, the unit might be dollars, or if the metric is measuring website latency, the unit might be milliseconds.

If the unit you're measuring on the x-axis is something you want to increase, such as revenue, account sign ups, and so on, then the farther to the right the curve is, the better. The variation with the curve farthest to the right means the unit the metric is measuring is highest for that variation.

If the unit you're measuring on the x-axis is something you want to decrease, such as website latency, then the farther to the left the curve is, the better. The variation with the curve farthest to the left means the unit the metric measuring is lowest for that variation.

How wide a curve is on the x-axis determines the credible interval. Narrower curves mean the results of the variation fall within a smaller range of values, so you can be more confident in the likely results of that variation's performance.

In the example below, the green variation has a more precise credible interval than the purple variation:

![An example experiment probability chart.](experiment-probability-chart-example.png)

To learn more, read [Credible interval](#credible-interval).

### The y-axis

The vertical y-axis measures probability. You can determine how probable it is that the metric will equal the number on the x-axis by how high the curve is. 

In the example above, the green variation has a high probability that the metric will measure 0.4 for any given context. In other words, if someone encounters the green variation, there's a high probability that the metric will measure 0.4 for that person.

### Viewing metric event values

In each metrics results table you can view metric event values per unique context as an average or a sum, depending on your needs.

Click the **View metric event values per unique context as** radio button to change your view selection:

![The "View metric event values per unique context as" option.](experiment-probability-chart-view-values-callout.png)

Here are examples of when to view metric event values by sum and by average:

- **Viewing event values as a sum:** Imagine you are tracking purchases made by customers using a new checkout flow. A customer makes three purchases for $15 each. If you view metric event values as an average, the average of their purchases is $15. If you view metric event values as a sum, the sum of their purchases is $45. If your goal is to increase revenue per customer, viewing metric event values as a sum makes the most sense.
- **Viewing event values as an average:** Imagine you want to decrease the time to checkout per transaction. If a customer takes two minutes for their first purchase, four minutes for their second purchase, and three minutes for their third purchase, their average checkout time is three minutes, but their total checkout time is nine minutes. In this case, viewing metric event values as an average makes the most sense.

## Results table

Each experiment has a results table with a row for each variation tested and a column for the variation's probability to be best, relative difference from the control, credible interval, and posterior mean:

![An experiment's results table.](experiment-results-table.png)

### Probability to be best

Probability to be best is the likelihood that a variation had the biggest effect on the primary metric. Of all the variations in the experiment, the one with highest probability is the best option to choose as a winner. 

In the above example, imagine that you are tracking the number of customers who complete a purchase on your website when offered a 15% off discount ("true") versus those who are offered no discount ("false"). The "true" variation has a probability to be best of 77%, while the "false" variation has a probability to be best of 23%, so you know the "true" variation is the best choice of the variations tested.

Use the "probability to be best" column to decide which variation is the winning variation in your experiment. LaunchDarkly provides additional advanced analysis for further information, but you do not need to use these statistics to make a decision about the winning variation. To learn more about how to choose a variation to roll out to your customers, read [Winning variations](/home/analyzing-experiments/winning-variation).

### Advanced analysis

<Callout intent="warning">
<CalloutTitle>This section includes advanced concepts</CalloutTitle>
<CalloutDescription>

This section includes an explanation of advanced statistical concepts. We provide them for informational purposes, but you do not need to understand these concepts in order to use Experimentation.

</CalloutDescription>
</Callout>

<Details summary="Expand Advanced analysis section">

### Relative difference from [the control]

The relative difference from the control variation is the difference between the mean of the control variation, and the upper and lower bounds of the credible interval of the variation you're testing. This range contains 90% of the variation's probable values. For example, imagine you have a control variation with a mean of 1%, and the variation you're testing has a lower credible interval of 1.1% and an upper credible interval of 1.5%. The difference between 1 and 1.1 is 10%, and the difference between 1 and 1.5 is 50%, so the treatment's relative difference from control is 10% to 50%.

In the above example, the "true" variation has a relative difference of [-7.41%, 35.01%]. You can have 90% confidence that the effect of the variation on the metric, for any given customer who encounters it, will be between -7.41% lower and 35.01% higher than the control variation. This means 90% of your customers who receive the 15% discount are checking out somewhere between 7.41% less and 35.01% more than those who receive no discount.

The longer you run an experiment, the more the width of this interval decreases. This is because the more data you gather, the more confidence you can have because the range of plausible values is smaller.

### Credible interval

The credible interval is the range that contains 90% of probable values.

This means the effect of the variation on the metric you're measuring has a 90% probability of falling between these two numbers. In the above example, the "true" variation has a credible interval of [0.57, 0.84]. This means you can have 90% confidence that the probable value of the metric, for any given customer who encounters it, will be between 0.57 and 0.84.

If you view metric event values per unique context as a sum instead of an average, the credible interval will be expressed in values rather than percentages. To learn more, read [Viewing metric event values](#viewing-metric-event-values).

The longer you run an experiment, the more the width of this interval decreases. This is because the more data you gather, the more confidence you can have because the range of plausible values is smaller.

### Posterior mean

The posterior mean is the average value of the posterior distribution of the variation. If you were to continue collecting data, the posterior mean is the future average numeric value for the variation that you should expect. It doesnâ€™t capture the uncertainty in the measurement, so it should not be the only measurement you use to make decisions.

In the above example, the posterior mean for "true" is 0.70 and the posterior mean for "false" is 0.62. If you continued to run the experiment, you could expect that offering a 15% discount in your store will result in customers checking out about 0.70% of the time, as opposed to 0.62% of the time with no discount.

All of the data in the results table are based on a posterior distribution. To learn more about posterior distributions, read [Frequentist and Bayesian modeling](/guides/experimentation/bayesian#frequentist-and-bayesian-modeling). LaunchDarkly automatically performs checks on the results data, to make sure that actual context traffic matches the allocation you set. To learn more, read [Understanding sample ratios](/home/analyzing-experiments/sample-ratios).

</Details>